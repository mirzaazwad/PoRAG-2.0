{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5dfb2c",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72432626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-11 05:28:18,311 - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b6a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    GenerationConfig,\n",
    ")\n",
    "import traceback\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import Document\n",
    "from rich import print as rprint\n",
    "from rich.panel import Panel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "\n",
    "CACHE_DIR = \"./models\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Default constants for the script\n",
    "show_context = True\n",
    "chat_model = \"asif00/bangla-llama-1B\"\n",
    "embed_model = \"l3cube-pune/bengali-sentence-similarity-sbert\"\n",
    "text_path = \"test.txt\"\n",
    "k = 4\n",
    "top_k = 2\n",
    "top_p = 0.6\n",
    "temperature = 0.6\n",
    "chunk_size = 500\n",
    "chunk_overlap = 150\n",
    "max_new_tokens = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdeba8",
   "metadata": {},
   "source": [
    "# Experiment with Asif Bangla Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be55f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "2025-05-11 00:43:08,242 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "মহাশ্বেতা দেবী একজন বিখ্যাত বাংলা কথাসাহিত্যিক মহাশ্বেতা হিসে\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model (after conversion)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asif00/bangla-llama-1B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"path/to/converted-model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"asif00/bangla-llama-1B\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "নিচের নির্দেশনা বাংলা ভাষায় যা একটি কাজ বর্ণনা করে, এবং ইনপুটও বাংলা ভাষায় যা অতিরিক্ত প্রসঙ্গ প্রদান করে। উপযুক্তভাবে অনুরোধ পূরণ করে বাংলা ভাষায় একটি প্রতিক্রিয়া লিখুন।\n",
    "\n",
    "### নির্দেশনা:\n",
    "{}\n",
    "\n",
    "### ইনপুট:\n",
    "{}\n",
    "\n",
    "### প্রতিক্রিয়া:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(instruction, context):\n",
    "    input_text = prompt.format(instruction, context)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### প্রতিক্রিয়া:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Example\n",
    "instruction = \"ভারতীয় বাঙালি কথাসাহিত্যিক মহাশ্বেতা দেবীর সম্পর্কে একটি সংক্ষিপ্ত বিবরণ দিন।\"\n",
    "context = \"মহাশ্বেতা দেবী ২০১৬ সালে হৃদরোগে আক্রান্ত হয়ে কলকাতায় মৃত্যুবরণ করেন।\"\n",
    "print(generate_response(instruction, context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39d5db",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8baafb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanglaRAGChain:\n",
    "    \"\"\"\n",
    "    Bangla Retrieval-Augmented Generation (RAG) Chain for question answering.\n",
    "\n",
    "    This class uses a HuggingFace/local language model for text generation, a Chroma vector database for\n",
    "    document retrieval, and a custom prompt template to create a RAG chain that can generate\n",
    "    responses to user queries in Bengali.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the BanglaRAGChain with default parameters.\"\"\"\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.chat_model_id = None\n",
    "        self.embed_model_id = None\n",
    "        self.k = 4\n",
    "        self.max_new_tokens = 1024\n",
    "        self.chunk_size = 500\n",
    "        self.chunk_overlap = 150\n",
    "        self.text_path = \"\"\n",
    "        self.temperature = 0.9\n",
    "        self.top_p = 0.6\n",
    "        self.top_k = 50\n",
    "        self._text_content = None\n",
    "        self.hf_token = None\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.chat_model = None\n",
    "        self._llm = None\n",
    "        self._retriever = None\n",
    "        self._db = None\n",
    "        self._documents = []\n",
    "        self._chain = None\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        chat_model_id,\n",
    "        embed_model_id,\n",
    "        text_path,\n",
    "        k=4,\n",
    "        top_k=2,\n",
    "        top_p=0.6,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.6,\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=150,\n",
    "        hf_token=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads the required models and data for the RAG chain.\n",
    "\n",
    "        Args:\n",
    "            chat_model_id (str): The Hugging Face model ID for the chat model.\n",
    "            embed_model_id (str): The Hugging Face model ID for the embedding model.\n",
    "            text_path (str): Path to the text file to be indexed.\n",
    "            k (int): The number of documents to retrieve.\n",
    "            top_k (int): The top_k parameter for the generation configuration.\n",
    "            top_p (float): The top_p parameter for the generation configuration.\n",
    "            max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "            temperature (float): The temperature parameter for the generation configuration.\n",
    "            chunk_size (int): The chunk size for text splitting.\n",
    "            chunk_overlap (int): The chunk overlap for text splitting.\n",
    "            hf_token (str): The Hugging Face token for authentication.\n",
    "        \"\"\"\n",
    "        self.chat_model_id = chat_model_id\n",
    "        self.embed_model_id = embed_model_id\n",
    "        self.k = k\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_path = text_path\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.hf_token = hf_token\n",
    "\n",
    "        if self.hf_token is not None:\n",
    "            os.environ[\"HF_TOKEN\"] = str(self.hf_token)\n",
    "\n",
    "        rprint(Panel(\"[bold green]Loading chat models...\", expand=False))\n",
    "        self._load_models()\n",
    "\n",
    "        rprint(Panel(\"[bold green]Creating document...\", expand=False))\n",
    "        self._create_document()\n",
    "\n",
    "        rprint(Panel(\"[bold green]Updating Chroma database...\", expand=False))\n",
    "        self._update_chroma_db()\n",
    "\n",
    "        rprint(Panel(\"[bold green]Initializing retriever...\", expand=False))\n",
    "        self._get_retriever()\n",
    "\n",
    "        rprint(Panel(\"[bold green]Initializing LLM...\", expand=False))\n",
    "        self._get_llm()\n",
    "        rprint(Panel(\"[bold green]Creating chain...\", expand=False))\n",
    "        self._create_chain()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"Loads the chat model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.chat_model_id)\n",
    "            self.chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.chat_model_id,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    cache_dir=CACHE_DIR\n",
    "                )\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]Error loading chat model: {e}\", expand=False))\n",
    "\n",
    "    def _create_document(self):\n",
    "        \"\"\"Splits the input text into chunks using RecursiveCharacterTextSplitter.\"\"\"\n",
    "        try:\n",
    "            with open(self.text_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                self._text_content = file.read()\n",
    "            character_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\"!\", \"?\", \"।\"],\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "            )\n",
    "            self._documents = list(\n",
    "                tqdm(\n",
    "                    character_splitter.split_text(self._text_content),\n",
    "                    desc=\"Chunking text\",\n",
    "                )\n",
    "            )\n",
    "            print(f\"Number of chunks: {len(self._documents)}\")\n",
    "            rprint(Panel(\"[bold green]Document created successfully!\", expand=False))\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]Chunking failed: {e}\", expand=False))\n",
    "\n",
    "    def _update_chroma_db(self):\n",
    "        \"\"\"Updates the Chroma vector database with the text chunks.\"\"\"\n",
    "        try:\n",
    "            rprint(Panel(\"[bold green]Loading embedding model...\", expand=False))\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=self.embed_model_id,\n",
    "                model_kwargs={\"device\": self._device},\n",
    "            )\n",
    "            rprint(Panel(\"[bold green]Embedding model loaded.\", expand=False))\n",
    "            docs = [Document(page_content=chunk) for chunk in self._documents]\n",
    "            self._db = Chroma.from_documents(\n",
    "                collection_name='porag_rag_collection',\n",
    "                persist_directory=CACHE_DIR,\n",
    "                documents=docs,\n",
    "                embedding=embeddings\n",
    "            )\n",
    "            self._db.persist()  \n",
    "            rprint(Panel(\"[bold green]Chroma DB created and persisted!\", expand=False))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            rprint(Panel(f\"[red]Vector DB initialization failed: {e}\", expand=False))\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "    def _create_chain(self):\n",
    "        \"\"\"Creates the retrieval-augmented generation (RAG) chain.\"\"\"\n",
    "        template = \"\"\"Below is an instruction in Bengali language that describes a task, paired with an input also in Bengali language that provides further context. Write a response in Bengali that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {question}\n",
    "\n",
    "        ### Input:\n",
    "        {context}\n",
    "\n",
    "        ### Response:\n",
    "        \"\"\"\n",
    "        prompt_template = ChatPromptTemplate(\n",
    "            input_variables=[\"question\", \"context\"],\n",
    "            output_parser=None,\n",
    "            partial_variables={},\n",
    "            messages=[\n",
    "                HumanMessagePromptTemplate(\n",
    "                    prompt=PromptTemplate(\n",
    "                        input_variables=[\"question\", \"context\"],\n",
    "                        output_parser=None,\n",
    "                        partial_variables={},\n",
    "                        template=template,\n",
    "                        template_format=\"f-string\",\n",
    "                        validate_template=True,\n",
    "                    ),\n",
    "                    additional_kwargs={},\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            rag_chain_from_docs = (\n",
    "                RunnablePassthrough.assign(\n",
    "                    context=lambda x: self._format_docs(x[\"context\"])\n",
    "                )\n",
    "                | prompt_template\n",
    "                | self._llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            rag_chain_with_source = RunnableParallel(\n",
    "                {\"context\": self._retriever, \"question\": RunnablePassthrough()}\n",
    "            ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "            self._chain = rag_chain_with_source\n",
    "            rprint(Panel(\"[bold green]RAG chain created successfully!\", expand=False))\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]RAG chain initialization failed: {e}\", expand=False))\n",
    "\n",
    "    def _get_llm(self):\n",
    "        \"\"\"Initializes the language model for the generation.\"\"\"\n",
    "        try:\n",
    "            config = GenerationConfig(\n",
    "                do_sample=True,\n",
    "                temperature=self.temperature,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                top_p=self.top_p,\n",
    "                top_k=self.top_k,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                bos_token_id=self.tokenizer.bos_token_id, \n",
    "            )\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.chat_model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                generation_config=config,\n",
    "            )\n",
    "            self._llm = HuggingFacePipeline(pipeline=pipe)\n",
    "            rprint(Panel(\"[bold green]LLM initialized successfully!\", expand=False))\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]LLM initialization failed: {e}\", expand=False))\n",
    "\n",
    "    def _get_retriever(self):\n",
    "        \"\"\"Initializes the retriever for document retrieval.\"\"\"\n",
    "        try:\n",
    "            self._retriever = self._db.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": self.k}\n",
    "            )\n",
    "            rprint(\n",
    "                Panel(\"[bold green]Retriever initialized successfully!\", expand=False)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]Retriever initialization failed: {e}\", expand=False))\n",
    "\n",
    "    def _format_docs(self, docs):\n",
    "        \"\"\"Formats the retrieved documents into a single string.\"\"\"\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def _clean_up(self, messages):\n",
    "        messages = re.sub(\"[^A-Za-z]+\", \"\", messages)\n",
    "        return messages\n",
    "\n",
    "    def get_response(self, query):\n",
    "        \"\"\"\n",
    "        Generates a response to the query using the RAG chain.\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the generated response and the retrieved context.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self._chain.invoke(query)\n",
    "            response_start = response[\"answer\"].find(\"### Response:\") + len(\n",
    "                \"### Response:\"\n",
    "            )\n",
    "            final_answer = response[\"answer\"][response_start:].strip()\n",
    "            if self._clean_up(final_answer):\n",
    "                self.get_response(query)\n",
    "\n",
    "            return final_answer, self._format_docs(response[\"context\"])\n",
    "        except Exception as e:\n",
    "            rprint(Panel(f\"[red]Answer generation failed: {e}\", expand=False))\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42251a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading chat models...</span> │\n",
       "╰────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────────────╮\n",
       "│ \u001b[1;32mLoading chat models...\u001b[0m │\n",
       "╰────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:24:58,804 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Creating document...</span> │\n",
       "╰──────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────╮\n",
       "│ \u001b[1;32mCreating document...\u001b[0m │\n",
       "╰──────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 7/7 [00:00<00:00, 66576.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Document created successfully!</span> │\n",
       "╰────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────────────────────╮\n",
       "│ \u001b[1;32mDocument created successfully!\u001b[0m │\n",
       "╰────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Updating Chroma database...</span> │\n",
       "╰─────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────╮\n",
       "│ \u001b[1;32mUpdating Chroma database...\u001b[0m │\n",
       "╰─────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading embedding model...</span> │\n",
       "╰────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────────────────╮\n",
       "│ \u001b[1;32mLoading embedding model...\u001b[0m │\n",
       "╰────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:25:06,873 - INFO - Load pretrained SentenceTransformer: l3cube-pune/bengali-sentence-similarity-sbert\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Embedding model loaded.</span> │\n",
       "╰─────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────╮\n",
       "│ \u001b[1;32mEmbedding model loaded.\u001b[0m │\n",
       "╰─────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:25:15,841 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Chroma DB created and persisted!</span> │\n",
       "╰──────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────╮\n",
       "│ \u001b[1;32mChroma DB created and persisted!\u001b[0m │\n",
       "╰──────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initializing retriever...</span> │\n",
       "╰───────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────────────────────╮\n",
       "│ \u001b[1;32mInitializing retriever...\u001b[0m │\n",
       "╰───────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Retriever initialized successfully!</span> │\n",
       "╰─────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────╮\n",
       "│ \u001b[1;32mRetriever initialized successfully!\u001b[0m │\n",
       "╰─────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initializing LLM...</span> │\n",
       "╰─────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────╮\n",
       "│ \u001b[1;32mInitializing LLM...\u001b[0m │\n",
       "╰─────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">LLM initialized successfully!</span> │\n",
       "╰───────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────────────────────────╮\n",
       "│ \u001b[1;32mLLM initialized successfully!\u001b[0m │\n",
       "╰───────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Creating chain...</span> │\n",
       "╰───────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────────────╮\n",
       "│ \u001b[1;32mCreating chain...\u001b[0m │\n",
       "╰───────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────╮\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">RAG chain created successfully!</span> │\n",
       "╰─────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────╮\n",
       "│ \u001b[1;32mRAG chain created successfully!\u001b[0m │\n",
       "╰─────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:25:21,581 - INFO - RAG model loaded successfully: chat_model=asif00/bangla-llama-1B, embed_model=l3cube-pune/bengali-sentence-similarity-sbert\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    rag_chain = BanglaRAGChain()\n",
    "    rag_chain.load(\n",
    "        chat_model_id=chat_model,\n",
    "        embed_model_id=embed_model,\n",
    "        text_path=text_path,\n",
    "        k=k,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        hf_token=hf_token,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"RAG model loaded successfully: chat_model={chat_model}, embed_model={embed_model}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Fatal error: {e}\")\n",
    "    print(\"Error occurred, please check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e0a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(query: str):\n",
    "    try:\n",
    "        answer, context = rag_chain.get_response(query)\n",
    "        if show_context:\n",
    "            print(f\"প্রসঙ্গঃ {context}\\n------------------------\\n\")\n",
    "        print(f\"উত্তর: {answer}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't generate an answer: {e}\")\n",
    "        print(\"আবার চেষ্টা করুন!\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Fatal error: {e}\")\n",
    "        print(\"Error occurred, please check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a184f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "প্রসঙ্গঃ ।\n",
      "\n",
      "শৈশব ও শিক্ষা\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর কলকাতার জোড়াসাঁকোর ঠাকুরবাড়িতে জন্মগ্রহণ করেন। তাঁর পিতা মহর্ষি দেবেন্দ্রনাথ ঠাকুর এবং মাতা সারদাসুন্দরী দেবী। তিনি বাড়িতেই প্রাথমিক শিক্ষা গ্রহণ করেন। বাড়িতে সংস্কৃত, ইংরেজি ও বাংলা ভাষায় শিক্ষা লাভ করেন। পরে ইংল্যান্ডে পাঠানো হয়, কিন্তু সেখান থেকে পুরো শিক্ষা সম্পূর্ণ না করে দেশে ফিরে আসেন।\n",
      "\n",
      "সাহিত্যিক কর্ম\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুরের সাহিত্যকর্মের পরিধি অত্যন্ত ব্যাপক। তিনি ৫২টি কাব্যগ্রন্থ, ৩৮টি নাটক, ১৩টি উপন্যাস এবং ৯৫টি ছোটগল্প লিখেছেন\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুরের জীবনী\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১) ছিলেন একজন বাঙালি কবি, সাহিত্যিক, নাট্যকার, সংগীতজ্ঞ, চিত্রশিল্পী এবং দার্শনিক। তিনি ভারতীয় সাহিত্য ও সঙ্গীতের প্রবাদপ্রতিম ব্যক্তিত্ব। ঠাকুর পরিবারের সমৃদ্ধ সাংস্কৃতিক পরিবেশে জন্ম নেওয়া রবীন্দ্রনাথ শৈশব থেকেই কবিতা ও গল্প লেখা শুরু করেন। তাঁর সাহিত্যকর্ম বাংলা সাহিত্যে বৈপ্লবিক পরিবর্তন আনে এবং সারা বিশ্বে সমাদৃত হয়।\n",
      "\n",
      "শৈশব ও শিক্ষা\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর কলকাতার জোড়াসাঁকোর ঠাকুরবাড়িতে জন্মগ্রহণ করেন\n",
      "\n",
      "।\n",
      "\n",
      "মৃত্যু ও উত্তরাধিকার\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর ১৯৪১ সালের ৭ আগস্ট কলকাতায় মৃত্যুবরণ করেন। তাঁর মৃত্যুতে বাংলা ও বিশ্বসাহিত্য এক বিরাট শূন্যতা অনুভব করে। তিনি তাঁর সাহিত্যকর্ম, সংগীত, চিত্রশিল্প, এবং দার্শনিক চিন্তাধারার মাধ্যমে আজও আমাদের মাঝে জীবন্ত রয়েছেন।\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুরের জীবনী আমাদের জানান দেয়, তিনি শুধুমাত্র একজন সাহিত্যিক ছিলেন না, বরং তিনি ছিলেন একজন সম্পূর্ণ মানুষ, যিনি তাঁর কর্মের মাধ্যমে সমগ্র বিশ্বের মানুষকে আলোকিত করেছেন।\n",
      "\n",
      "।\n",
      "\n",
      "শিক্ষা ও সমাজসেবা\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর শিক্ষাক্ষেত্রেও অনন্য অবদান রাখেন। তিনি পশ্চিমবঙ্গের বীরভূম জেলার শান্তিনিকেতনে \"বিশ্বভারতী বিশ্ববিদ্যালয়\" প্রতিষ্ঠা করেন। এই বিশ্ববিদ্যালয় ভারতীয় এবং পাশ্চাত্য শিক্ষা ও সংস্কৃতির সমন্বয়ে গড়ে তোলা হয়। ঠাকুর সমাজসেবা এবং সামাজিক সংস্কারেও সক্রিয় ভূমিকা পালন করেন।\n",
      "\n",
      "মৃত্যু ও উত্তরাধিকার\n",
      "\n",
      "রবীন্দ্রনাথ ঠাকুর ১৯৪১ সালের ৭ আগস্ট কলকাতায় মৃত্যুবরণ করেন। তাঁর মৃত্যুতে বাংলা ও বিশ্বসাহিত্য এক বিরাট শূন্যতা অনুভব করে\n",
      "------------------------\n",
      "\n",
      "উত্তর: রবীন্দ্রনাথ ঠাকুরের জন্মস্থান কলকাতার জোড়াসাঁকোর ঠাকুরবাড়ি।\n"
     ]
    }
   ],
   "source": [
    "prompt(\"রবীন্দ্রনাথ ঠাকুরের জন্মস্থান কোথায়?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banglarag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
